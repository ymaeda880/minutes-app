# -*- coding: utf-8 -*-
# pages/11_é‡è¤‡ç®‡æ‰€æ¤œå‡º2.py
#
# å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’ã‚­ãƒ¼ã«ã€
# å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ï¼ˆæ–‡å­—æ•°ï¼‰ã€ã®ä¸­ã‹ã‚‰
# ã‚‚ã£ã¨ã‚‚ã‚ˆãä¸€è‡´ã™ã‚‹ä½ç½®ã‚’æ¢ã—ã€
# å‰åŠå´ã«ã ã‘ã€Œ-----ã“ã“ã‹ã‚‰é‡è¤‡-----ã€ã‚’æŒ¿å…¥ã™ã‚‹ã€‚
#
# å¾ŒåŠå´ã«ã¯ã€Œã“ã“ã¾ã§ãŒé‡è¤‡éƒ¨åˆ†ã€ã¯å…¥ã‚Œãªã„ã€‚

from __future__ import annotations

import re
from difflib import SequenceMatcher
from typing import List, Dict, Tuple

import streamlit as st

# ============================================================
# è¨­å®šå€¤
# ============================================================

# ã€Œå‰å¾Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ¯”è¼ƒç¯„å›²ã€ï¼ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—é•·ï¼ˆå‰åŠã®æœ€å¾Œã®ä½•æ–‡å­—ã‚’è¦‹ã‚‹ã‹ï¼‰
OVERLAP_CHARS = 700  # ãŠãŠã‚ˆã2åˆ†ç›¸å½“ã®ç›®å®‰

# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã¨ã—ã¦ä½¿ã†æœ€å¤§æ–‡å­—æ•°
HEAD_CHARS = 400

# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã¨ã—ã¦åŒºåˆ‡ã‚‹æœ€å¤§æ–‡æ•°
HEAD_SENTENCES = 3

# ä¸€è‡´ã¨ã¿ãªã™æœ€ä½æ–‡å­—æ•°
MIN_MATCH_SIZE = 20

# å¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼è¡Œ
MARKER_PATTERN = re.compile(
    r"^-{3,}\s*ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆ(.*?)ï¼‰.*$",
    re.MULTILINE,
)

BEGIN_TAG = "-----ã“ã“ã‹ã‚‰é‡è¤‡-----"


# ============================================================
# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²
# ============================================================

def split_by_markers(text: str) -> Tuple[List[str], List[Dict[str, str]]]:
    """
    å…¨æ–‡ã‹ã‚‰ã€Œã¤ãªãç›®ãƒãƒ¼ã‚«ãƒ¼è¡Œã€ã§åˆ†å‰²ã—ã¦
    segments ã¨ markers ã‚’è¿”ã™ã€‚

    [seg0][marker0][seg1][marker1][seg2]...
    â†’ segments = [seg0, seg1, seg2, ...]
      markers  = [marker0, marker1, ...]
    """
    segments: List[str] = []
    markers: List[Dict[str, str]] = []

    prev_end = 0

    for m in MARKER_PATTERN.finditer(text):
        seg = text[prev_end:m.start()]
        segments.append(seg)

        markers.append(
            {
                "file_name": m.group(1),
                "marker_text": m.group(0),
            }
        )
        prev_end = m.end()

    segments.append(text[prev_end:])
    return segments, markers


# ============================================================
# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã‚’æŠœãå‡ºã™
# ============================================================

def extract_head_phrase(next_seg: str) -> str:
    """
    å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’å–ã‚Šå‡ºã™ã€‚

    - å…ˆé ­ HEAD_CHARS æ–‡å­—ã‚’å¯¾è±¡
    - ãã®ä¸­ã§ã€Œã€‚ã€ã€Œï¼Ÿã€ã€Œï¼ã€ã€Œæ”¹è¡Œã€ã‚’æ–‡ã®åŒºåˆ‡ã‚Šã¨ã¿ãªã—ã€
      HEAD_SENTENCES æ–‡ã¾ã§å«ã‚ãŸéƒ¨åˆ†ã‚’ head_phrase ã¨ã—ã¦è¿”ã™ã€‚
    """
    if not next_seg:
        return ""

    s = next_seg[:HEAD_CHARS]
    count = 0
    end = len(s)

    for i, ch in enumerate(s):
        if ch in "ã€‚ï¼Ÿï¼\n":
            count += 1
            if count >= HEAD_SENTENCES:
                end = i + 1  # åŒºåˆ‡ã‚Šè¨˜å·ã‚‚å«ã‚ã‚‹
                break

    return s[:end]


# ============================================================
# é‡è¤‡é–‹å§‹ä½ç½®ã®æ¢ç´¢ï¼ˆå‰åŠå´ã®ã¿ï¼‰
# ============================================================

# æ­£è¦åŒ–ï¼šå¥èª­ç‚¹ãƒ»æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ï¼ˆå¿…è¦ãªã‚‰ã•ã‚‰ã«è¿½åŠ å¯ï¼‰
def normalize_text(s: str) -> str:
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ â†’ ç©º
    s = s.replace("ã€€", "")
    # æ”¹è¡Œ â†’ ç©º
    s = s.replace("\n", "")
    # å¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å»
    s = re.sub(r"[ã€ã€‚ï¼ï¼Ÿ,.!?]", "", s)
    # ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹é™¤å»
    s = s.replace(" ", "")
    return s


def find_overlap_start(prev_seg: str, next_seg: str) -> Tuple[int, int]:
    """
    æ”¹è‰¯ç‰ˆï¼š
      - MIN_MATCH_SIZE = 20 ã«å¤‰æ›´
      - ä¸¡æ–¹ã®æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ã—ã¦ã‹ã‚‰ SequenceMatcher ã§ä¸€è‡´ä½ç½®ã‚’æ¤œå‡º
      - ãã®ä½ç½®ã‚’å…ƒã®éæ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆã® index ã«æˆ»ã—ã¦è¿”ã™
    """

    if not prev_seg or not next_seg:
        return -1, 0

    from_head = extract_head_phrase(next_seg)
    if len(from_head) < MIN_MATCH_SIZE:
        return -1, 0

    # å‰åŠå´ã®æœ«å°¾
    prev_tail = prev_seg[-OVERLAP_CHARS:]

    # æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    norm_head = normalize_text(from_head)
    norm_prev = normalize_text(prev_tail)

    if len(norm_head) < MIN_MATCH_SIZE:
        return -1, 0

    # é¡ä¼¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
    sm = SequenceMatcher(None, norm_head, norm_prev)
    blocks = sm.get_matching_blocks()

    # å€™è£œæŠ½å‡º
    cand = [(b.a, b.b, b.size) for b in blocks if b.size >= MIN_MATCH_SIZE]
    if not cand:
        return -1, 0

    # å„ªå…ˆé †ä½ï¼š
    #   head_phrase å´ã®é–‹å§‹ãŒå…ˆ â†’ prev_tail å´ã®é–‹å§‹ãŒå…ˆ â†’ size ãŒå¤§ãã„
    a, b, size = sorted(cand, key=lambda t: (t[0], t[1], -t[2]))[0]

    # ======== å…ƒãƒ†ã‚­ã‚¹ãƒˆã® index ã«æˆ»ã™ãŸã‚ã®å‡¦ç† ========

    # æ­£è¦åŒ–å‰ã® from_head ã¨ prev_tail ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œã‚‹
    def build_index_map(raw: str, norm: str):
        """
        æ­£è¦åŒ–å‰ raw ã®å„æ–‡å­—ãŒã€æ­£è¦åŒ–å¾Œ norm ã®
        ã©ã® index ã«å¯¾å¿œã™ã‚‹ã‹ã‚’è¿”ã™ map
        """
        mapping = []
        j = 0
        for i, ch in enumerate(raw):
            # æ­£è¦åŒ–ã§æ¶ˆãˆã‚‹æ–‡å­—ã¯ mapping ã«å…¥ã‚Œãªã„
            ch_norm = normalize_text(ch)
            if ch_norm == "":
                continue
            if j < len(norm):
                mapping.append((j, i))
                j += 1
        return mapping

    # from_head â†’ æ­£è¦åŒ–ç‰ˆã® index map
    head_map = build_index_map(from_head, norm_head)
    prev_map = build_index_map(prev_tail, norm_prev)

    # a, b ã¯æ­£è¦åŒ–å¾Œã® index ãªã®ã§
    # raw å´ index ã«é€†å¤‰æ›ã™ã‚‹
    # æœ€ã‚‚è¿‘ã„ raw index ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    def mapped_index(mapping, idx_norm):
        # idx_norm ã«æœ€ã‚‚è¿‘ã„ mapping ã® raw index ã‚’è¿”ã™
        candidates = [raw_idx for norm_idx, raw_idx in mapping if norm_idx == idx_norm]
        if candidates:
            return candidates[0]
        # ç›´æ¥ä¸€è‡´ãŒãªã‘ã‚Œã°è¿‘ã„ã‚‚ã®ã‚’æ¢ã™
        nearest = None
        best_dist = 10**9
        for norm_idx, raw_idx in mapping:
            d = abs(norm_idx - idx_norm)
            if d < best_dist:
                best_dist = d
                nearest = raw_idx
        return nearest

    # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®é–‹å§‹ä½ç½®
    head_raw_start = mapped_index(head_map, a)
    prev_raw_start = mapped_index(prev_map, b)

    # ã€Œhead ã®å…ˆé ­ã¨ align ã™ã‚‹ã€å‰æã§è£œæ­£
    start_in_tail = max(0, prev_raw_start - head_raw_start)

    global_prev_start = len(prev_seg) - len(prev_tail) + start_in_tail

    return global_prev_start, size


# ============================================================
# çµåˆå‡¦ç†ï¼ˆå‰åŠå´ã ã‘ãƒãƒ¼ã‚¯ï¼‰
# ============================================================

def build_merged_text(text: str) -> str:
    """
    ã¤ãªãç›®ã”ã¨ã«å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœ«å°¾ã®é‡è¤‡é–‹å§‹ä½ç½®ã‚’æ¢ã—ã€
    ãã“ã«ã€Œ-----ã“ã“ã‹ã‚‰é‡è¤‡-----ã€ã‚’æŒ¿å…¥ã™ã‚‹ã€‚

    å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ãã®ã¾ã¾ï¼ˆã€Œã“ã“ã¾ã§é‡è¤‡ã€ã¯ä»˜ã‘ãªã„ï¼‰ã€‚
    """
    segments, markers = split_by_markers(text)

    if len(segments) <= 1 or not markers:
        # ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        return text

    merged: List[str] = []

    # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¸€æ—¦ãã®ã¾ã¾å…¥ã‚Œã¦ãŠã
    merged.append(segments[0])

    for idx, marker in enumerate(markers):
        prev_seg = segments[idx]
        next_seg = segments[idx + 1]

        # é‡è¤‡é–‹å§‹ä½ç½®ã‚’æ¢ç´¢
        start_pos, size = find_overlap_start(prev_seg, next_seg)

        if start_pos < 0 or size <= 0:
            # é‡è¤‡ãŒæ¤œå‡ºã§ããªã‹ã£ãŸå ´åˆï¼š
            #   ç›´å‰ã«å…¥ã‚Œã¦ã„ãŸå‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ãã®ã¾ã¾ã€
            #   ãƒãƒ¼ã‚«ãƒ¼ã¨å¾Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãã®ã¾ã¾ç¹‹ã
            merged.append("\n" + marker["marker_text"] + "\n")
            merged.append(next_seg)
            continue

        # å‰åŠå´ã®é‡è¤‡é–‹å§‹ä½ç½®ã§ã‚¿ã‚°ã‚’æŒ¿å…¥ã—ãŸæ–°ã—ã„å‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½œã‚‹
        new_prev = (
            prev_seg[:start_pos]
            + "\n" + BEGIN_TAG + "\n"
            + prev_seg[start_pos:]
        )

        # ç›´å‰ã® merged[-1]ï¼ˆå¤ã„ prev_segï¼‰ã‚’å·®ã—æ›¿ãˆ
        merged[-1] = new_prev

        # ãƒãƒ¼ã‚«ãƒ¼è¡Œ
        merged.append("\n" + marker["marker_text"] + "\n")

        # å¾ŒåŠå´ã¯ä½•ã‚‚å‰Šã‚‰ãšã€ãã®ã¾ã¾ç¶šã‘ã‚‹
        merged.append(next_seg)

    return "".join(merged)


# ============================================================
# Streamlit UI
# ============================================================

st.set_page_config(
    page_title="ğŸ“ æ–‡å­—èµ·ã“ã—çµåˆï¼ˆé‡è¤‡æ¤œå‡ºï¼šå¾ŒåŠã®æœ€åˆã®æ•°è¡Œãƒ™ãƒ¼ã‚¹ï¼‰",
    page_icon="ğŸ“",
    layout="wide",
)

st.title("ğŸ“ é‡è¤‡ç®‡æ‰€æ¤œå‡ºï¼ˆå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã‹ã‚‰é‡è¤‡é–‹å§‹ã‚’æ¤œå‡ºï¼‰")

st.markdown(
    """
- å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã® **ã€Œæœ€åˆã®æ•°è¡Œã€** ã‚’ã‚­ãƒ¼ã«ã—ã¦ã€  
  å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã® **ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ã€** ã®ä¸­ã‹ã‚‰  
  ã‚‚ã£ã¨ã‚‚ã‚ˆãä¸€è‡´ã™ã‚‹ä½ç½®ã‚’æ¢ã—ã€å‰åŠå´ã«ã ã‘  
  `-----ã“ã“ã‹ã‚‰é‡è¤‡-----` ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚

- å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã¯ã€Œã“ã“ã¾ã§é‡è¤‡éƒ¨åˆ†ã€ã¯æŒ¿å…¥ã—ã¾ã›ã‚“ã€‚
"""
)

# ğŸ”½ ãƒ­ã‚¸ãƒƒã‚¯èª¬æ˜ï¼ˆåˆæœŸã¯ç•³ã‚“ã§ãŠãï¼‰
with st.expander("ğŸ” ã“ã®ãƒ„ãƒ¼ãƒ«ã®é‡è¤‡æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ¦‚è¦ï¼‰", expanded=False):
    st.markdown(
        """
1. **ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²**  
   - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’ã€Œ--- ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆâ€¦ï¼‰ã€ã¨ã„ã†ãƒãƒ¼ã‚«ãƒ¼è¡Œã§åˆ†å‰²ã—ã€  
     `[å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆ][ãƒãƒ¼ã‚«ãƒ¼][å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆ]...` ã¨ã„ã†å½¢ã«åˆ†ã‘ã¾ã™ã€‚

2. **å¾ŒåŠå´ã®ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’æŠ½å‡º**  
   - å„ã¤ãªãç›®ã«ã¤ã„ã¦ã€å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å…ˆé ­ã‹ã‚‰ `HEAD_CHARS` æ–‡å­—ã‚’å–ã‚Šå‡ºã—ã€  
     ã€Œã€‚ã€ã€Œï¼Ÿã€ã€Œï¼ã€ã€Œæ”¹è¡Œã€ãªã©ã§åŒºåˆ‡ã£ã¦ **æœ€å¤§ `HEAD_SENTENCES` æ–‡** ã¾ã§ã‚’  
     `head_phrase` ã¨ã—ã¦ä½¿ã„ã¾ã™ã€‚

3. **å‰åŠå´ã®ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ã€ã‚’å–å¾—**  
   - å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ«å°¾ã‹ã‚‰ `OVERLAP_CHARS` æ–‡å­—ã ã‘ã‚’åˆ‡ã‚Šå‡ºã—ã€  
     ã“ã‚Œã‚’ `prev_tail` ã¨ã—ã¦æ¯”è¼ƒå¯¾è±¡ã«ã—ã¾ã™ã€‚

4. **æ­£è¦åŒ–ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—**  
   - `head_phrase` ã¨ `prev_tail` ã‹ã‚‰  
     - å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹  
     - æ”¹è¡Œ  
     - å¥èª­ç‚¹ï¼ˆã€ã€‚ï¼ï¼Ÿ,.!?ï¼‰  
     - åŠè§’ã‚¹ãƒšãƒ¼ã‚¹  
     ã‚’å–ã‚Šé™¤ã„ãŸ **æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆ** ã‚’ä½œã‚Šã¾ã™ã€‚
   - æ­£è¦åŒ–å¾Œã®æ–‡å­—åˆ—åŒå£«ã«å¯¾ã—ã¦ `SequenceMatcher` ã‚’ç”¨ã„ã¦ä¸€è‡´ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—ã—ã€  
     **20æ–‡å­—ä»¥ä¸Šï¼ˆ`MIN_MATCH_SIZE = 20`ï¼‰é€£ç¶šã—ã¦ä¸€è‡´ã™ã‚‹éƒ¨åˆ†**ã‚’å€™è£œã¨ã—ã¾ã™ã€‚

5. **ã‚‚ã£ã¨ã‚‚è‡ªç„¶ãªä¸€è‡´ä½ç½®ã‚’é¸ã¶**  
   - å€™è£œã®ä¸­ã‹ã‚‰  
     1. å¾ŒåŠå´ï¼ˆhead_phraseï¼‰ã®å…ˆé ­ã«è¿‘ã„  
     2. å‰åŠå´ï¼ˆprev_tailï¼‰ã®å…ˆé ­ã«è¿‘ã„  
     3. ä¸€è‡´é•·ãŒé•·ã„  
     ã‚‚ã®ã‚’å„ªå…ˆã—ã¦ 1 ä»¶æ¡ç”¨ã—ã¾ã™ã€‚

6. **æ­£è¦åŒ–å‰ã®ä½ç½®ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ã‚¿ã‚°ã‚’æŒ¿å…¥**  
   - æ­£è¦åŒ–å‰ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ƒã®æ–‡å­—åˆ—ï¼‰ã¨æ­£è¦åŒ–å¾Œãƒ†ã‚­ã‚¹ãƒˆã®  
     **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾å¿œè¡¨**ã‚’ä½œã‚Šã€  
     æ­£è¦åŒ–å¾Œã§è¦‹ã¤ã‹ã£ãŸä¸€è‡´ä½ç½®ã‚’ **å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æˆ»ã—ã¾ã™**ã€‚
   - å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãã®ä½ç½®ã«  
     `-----ã“ã“ã‹ã‚‰é‡è¤‡-----`  
     ã¨ã„ã†è¡Œã‚’æŒ¿å…¥ã—ã€å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆå´ã¯ä¸€åˆ‡å‰Šã‚‰ãšãã®ã¾ã¾é€£çµã—ã¾ã™ã€‚
        """
    )


uploaded_files = st.file_uploader(
    "æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ (.txt) ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["txt"],
    accept_multiple_files=True,
)

run = st.button("â–¶ï¸ é‡è¤‡ç®‡æ‰€æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹", type="primary")

if run:
    if not uploaded_files:
        st.warning("å…ˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    else:
        for up in uploaded_files:
            name = up.name
            raw = up.read()
            try:
                text = raw.decode("utf-8")
            except UnicodeDecodeError:
                text = raw.decode("cp932", errors="replace")

            merged = build_merged_text(text)

            st.subheader(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {name}")
            with st.expander("ğŸ“˜ çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=True):
                st.text(merged)

            st.download_button(
                "ğŸ“¥ çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.txt)",
                merged.encode("utf-8"),
                file_name=f"{name.rsplit('.',1)[0]}_é‡è¤‡ç®‡æ‰€æ¤œå‡º.txt",
                mime="text/plain",
            )

        st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
