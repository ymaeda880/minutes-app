# -*- coding: utf-8 -*-
# pages/30_é‡è¤‡ç®‡æ‰€æ¤œå‡º_storageå¯¾å¿œ.py
#
# âœ… storageå¯¾å¿œç‰ˆï¼ˆãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆï¼‰
# - ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªï¼ˆpages/13 ã¨åŒã˜ï¼‰
# - Storages/<user>/minutes_app/ é…ä¸‹ã®ã€Œé€£çµæ–‡å­—èµ·ã“ã—ï¼ˆcombined txtï¼‰ã€ã‚’åˆ—æŒ™
# - radio ã§ 1ã¤é¸æŠ â†’ é‡è¤‡æ¤œå‡ºï¼ˆå‰åŠå´ã«ã ã‘ BEGIN_TAG ã‚’æŒ¿å…¥ï¼‰
# - çµæœï¼ˆmerged txtï¼‰ï¼‹æ¤œå‡ºãƒ­ã‚°ï¼ˆjsonï¼‰ã‚’ transcript/ ã«ä¿å­˜
#
# â€» common_lib ã¯æ”¹å¤‰ã—ãªã„
# â€» use_container_width ã¯ä½¿ã‚ãªã„ï¼ˆæ–¹é‡ã«å¾“ã†ï¼‰

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
import sys

import streamlit as st

_THIS = Path(__file__).resolve()
PROJECTS_ROOT = _THIS.parents[3]

if str(PROJECTS_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECTS_ROOT))

from common_lib.storage.external_ssd_root import resolve_storage_subdir_root
from common_lib.auth.auth_helpers import require_login


# ============================================================
# è¨­å®šå€¤ï¼ˆæ—¢å­˜ï¼‰
# ============================================================

OVERLAP_CHARS = 700
HEAD_CHARS = 400
HEAD_SENTENCES = 3
DEFAULT_MIN_MATCH_SIZE = 20
HEAD_SHIFT_TRIES = 3

MARKER_PATTERN = re.compile(
    r"^-{3,}\s*ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆ(.*?)ï¼‰.*$",
    re.MULTILINE,
)

BEGIN_TAG = "-----ã“ã“ã‹ã‚‰é‡è¤‡-----"


# ============================================================
# pathsï¼ˆPROJECTS_ROOT åŸºæº–ï¼‰
# ============================================================
STORAGE_ROOT = resolve_storage_subdir_root(
    PROJECTS_ROOT,
    subdir="Storages",
)


# ============================================================
# utils
# ============================================================
def safe_mkdir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def write_text(p: Path, s: str) -> None:
    safe_mkdir(p.parent)
    p.write_text(s, encoding="utf-8")


def write_json(p: Path, obj: Any) -> None:
    safe_mkdir(p.parent)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def append_log(log_path: Path, msg: str) -> None:
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    safe_mkdir(log_path.parent)
    with log_path.open("a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")


def _sanitize_username_for_path(username: str) -> str:
    u = (username or "").strip()
    if not u:
        return "anonymous"
    u = re.sub(r"[^0-9A-Za-z_-]+", "_", u).strip("_")
    return u or "anonymous"


def _human_dt(s: str | None) -> str:
    if not s:
        return "â€”"
    try:
        return s.replace("T", " ").replace("+00:00", "Z")
    except Exception:
        return s


# ============================================================
# storage: combined txt listing
#   Storages/<user>/minutes_app/<YYYY-MM-DD>/job_*/transcript_speaker_separated_combined/*.txt
# ============================================================
@dataclass
class CombinedItem:
    label: str
    path: Path
    job_dir: Path
    transcript_dir: Path
    job_id: str
    date: str
    created_at: Optional[str]


def _read_job_json(job_dir: Path) -> dict:
    p = job_dir / "job.json"
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}

def list_combined_texts(user_dir: str) -> list[CombinedItem]:
    base = STORAGE_ROOT / user_dir / "minutes_app"
    if not base.exists():
        return []

    items: list[CombinedItem] = []

    # æ—¥ä»˜é™é †
    for day_dir in sorted(base.glob("*"), reverse=True):
        if not day_dir.is_dir():
            continue

        # jobé™é †
        for job_dir in sorted(day_dir.glob("job_*"), reverse=True):
            if not job_dir.is_dir():
                continue

            meta = _read_job_json(job_dir)
            job_id = str(meta.get("job_id") or job_dir.name)
            date = str(meta.get("date") or day_dir.name)
            created_at = meta.get("created_at")

            # å…¥åŠ›å…ƒï¼štranscript_speaker_separated_combined
            combined_dir = job_dir / "transcript_speaker_separated_combined"
            if not combined_dir.exists():
                continue

            combined_files = sorted(
                combined_dir.glob("*.txt"),
                key=lambda p: p.name.lower(),
                reverse=True,
            )
            for p in combined_files:
                label = f"{date} / {job_id} / {p.name} / created={_human_dt(created_at)}"
                items.append(
                    CombinedItem(
                        label=label,
                        path=p,
                        job_dir=job_dir,
                        transcript_dir=combined_dir,
                        job_id=job_id,
                        date=date,
                        created_at=created_at,
                    )
                )

    return items



# ============================================================
# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ï¼ˆæ—¢å­˜ï¼‰
# ============================================================
def split_by_markers(text: str) -> Tuple[List[str], List[Dict[str, str]]]:
    segments: List[str] = []
    markers: List[Dict[str, str]] = []
    prev_end = 0

    for m in MARKER_PATTERN.finditer(text):
        seg = text[prev_end:m.start()]
        segments.append(seg)
        markers.append({"file_name": m.group(1), "marker_text": m.group(0)})
        prev_end = m.end()

    segments.append(text[prev_end:])
    return segments, markers


# ============================================================
# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã‚’æŠœãå‡ºã™ï¼ˆæ—¢å­˜ï¼‰
# ============================================================
# === è¿½åŠ ï¼šè©±è€…ãƒ©ãƒ™ãƒ«ï¼ˆè¡Œé ­ï¼‰é™¤å» ===
SPEAKER_PREFIX_PATTERN = re.compile(
    r"""^(
        \s*                                   # å…ˆé ­ç©ºç™½
        (?:å¸ä¼š|ï¼­ï¼£|MC|é€²è¡Œ)                  # æ—¥æœ¬èª/MCç³»ãƒ©ãƒ™ãƒ«ï¼ˆå¿…è¦ãªã‚‰å¢—ã‚„ã™ï¼‰
        \s*[:ï¼š]\s*                            # ã‚³ãƒ­ãƒ³
      |
        \s*\[?\s*[sS]\s*\d+\s*\]?\s*[:ï¼š]\s*   # S12: / [s12]: / S 12 :
    )""",
    re.VERBOSE,
)

def strip_leading_speaker_labels(text: str) -> str:
    """
    ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå…ˆé ­ã«ä»˜ã„ãŸè©±è€…ãƒ©ãƒ™ãƒ«ï¼ˆS4: / [s4]: / å¸ä¼š: ç­‰ï¼‰ã‚’å‰¥ãŒã™ã€‚
    å…ˆé ­ã‹ã‚‰é€£ç¶šã—ã¦ä»˜ãã‚±ãƒ¼ã‚¹ã‚‚ã‚ã‚‹ã®ã§ç¹°ã‚Šè¿”ã—é™¤å»ã™ã‚‹ã€‚
    """
    if not text:
        return ""
    s = text.lstrip("\ufeff")  # å¿µã®ãŸã‚BOMé™¤å»
    # å…ˆé ­è¡Œã«é€£ç¶šã§ä»˜ãå ´åˆã‚’è€ƒæ…®ã—ã¦ãƒ«ãƒ¼ãƒ—
    for _ in range(5):  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        m = SPEAKER_PREFIX_PATTERN.match(s)
        if not m:
            break
        s = s[m.end():]
    return s


def extract_head_phrase(next_seg: str) -> str:
    if not next_seg:
        return ""

    # â˜… è¿½åŠ ï¼šå…ˆé ­ã®è©±è€…ãƒ©ãƒ™ãƒ«ã‚’å‰¥ãŒã—ã¦ã‹ã‚‰ã‚­ãƒ¼æ–‡ã‚’ä½œã‚‹
    next_seg = strip_leading_speaker_labels(next_seg)

    s = next_seg[:HEAD_CHARS]
    count = 0
    end = len(s)

    for i, ch in enumerate(s):
        if ch in "ã€‚ï¼Ÿï¼\n":
            count += 1
            if count >= HEAD_SENTENCES:
                end = i + 1
                break

    return s[:end]



# ============================================================
# æ­£è¦åŒ– & ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ—¢å­˜ï¼‰
# ============================================================
def normalize_text(s: str) -> str:
    s = s.replace("ã€€", "")
    s = s.replace("\n", "")
    s = re.sub(r"[ã€ã€‚ï¼ï¼Ÿ,.!?]", "", s)
    s = s.replace(" ", "")
    return s


def _match_with_phrase(
    prev_seg: str,
    phrase: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[int, int]:
    if not prev_seg or not phrase:
        return -1, 0
    if len(phrase) < min_match_size:
        return -1, 0

    prev_tail = prev_seg[-OVERLAP_CHARS:]
    norm_head = normalize_text(phrase)
    norm_prev = normalize_text(prev_tail)

    if len(norm_head) < min_match_size:
        return -1, 0

    sm = SequenceMatcher(None, norm_head, norm_prev, autojunk=use_autojunk)
    blocks = sm.get_matching_blocks()

    cand = [(b.a, b.b, b.size) for b in blocks if b.size >= min_match_size]
    if not cand:
        return -1, 0

    a, b, size = sorted(cand, key=lambda t: (t[0], t[1], -t[2]))[0]

    def build_index_map(raw: str, norm: str):
        mapping = []
        j = 0
        for i, ch in enumerate(raw):
            ch_norm = normalize_text(ch)
            if ch_norm == "":
                continue
            if j < len(norm):
                mapping.append((j, i))
                j += 1
        return mapping

    head_map = build_index_map(phrase, norm_head)
    prev_map = build_index_map(prev_tail, norm_prev)

    def mapped_index(mapping, idx_norm):
        candidates = [raw_idx for norm_idx, raw_idx in mapping if norm_idx == idx_norm]
        if candidates:
            return candidates[0]
        nearest = None
        best_dist = 10**9
        for norm_idx, raw_idx in mapping:
            d = abs(norm_idx - idx_norm)
            if d < best_dist:
                best_dist = d
                nearest = raw_idx
        return nearest

    head_raw_start = mapped_index(head_map, a)
    prev_raw_start = mapped_index(prev_map, b)

    start_in_tail = max(0, int(prev_raw_start) - int(head_raw_start))
    global_prev_start = len(prev_seg) - len(prev_tail) + start_in_tail
    return global_prev_start, int(size)


def find_overlap_start(
    prev_seg: str,
    next_seg: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[int, int, str, List[str], str]:
    if not prev_seg or not next_seg:
        return -1, 0, "", [], ""

    raw_head = extract_head_phrase(next_seg)
    if not raw_head:
        return -1, 0, "", [], ""

    base_head = raw_head.strip().rstrip("ã€‚ï¼Ÿï¼!ï¼Ÿï¼Œã€,.")
    if not base_head:
        return -1, 0, "", [], ""

    candidates: List[str] = []
    seen: set[str] = set()
    shifted_heads: List[str] = []

    def add_candidate(s: str, is_shifted: bool = False):
        if not s:
            return
        if s in seen:
            return
        seen.add(s)
        candidates.append(s)
        if is_shifted:
            shifted_heads.append(s)

    add_candidate(base_head, is_shifted=False)

    current = base_head
    for _ in range(HEAD_SHIFT_TRIES):
        if len(current) <= 1:
            break
        current = current[1:]
        add_candidate(current, is_shifted=True)

    matched_phrase = ""
    for phrase in candidates:
        start_pos, size = _match_with_phrase(prev_seg, phrase, min_match_size, use_autojunk)
        if start_pos >= 0 and size > 0:
            matched_phrase = phrase
            return start_pos, size, base_head, shifted_heads, matched_phrase

    return -1, 0, base_head, shifted_heads, ""


def build_merged_text(
    text: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[str, List[Dict[str, Any]]]:
    segments, markers = split_by_markers(text)

    if len(segments) <= 1 or not markers:
        return text, []

    merged: List[str] = []
    logs: List[Dict[str, Any]] = []

    merged.append(segments[0])

    for idx, marker in enumerate(markers):
        prev_seg = segments[idx]
        next_seg = segments[idx + 1]

        start_pos, size, base_head, shifted_heads, matched_phrase = find_overlap_start(
            prev_seg, next_seg, min_match_size, use_autojunk
        )

        if start_pos < 0 or size <= 0:
            logs.append(
                {
                    "ã¤ãªãç›®ç•ªå·": idx,
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": marker.get("file_name", ""),
                    "æ¤œå‡ºçµæœ": "è¦‹ã¤ã‹ã‚‰ãš",
                    "é–‹å§‹ä½ç½®": None,
                    "ä¸€è‡´æ–‡å­—æ•°": 0,
                    "head_phrase": base_head,
                    "shifted_phrases": shifted_heads,
                    "matched_phrase": "",
                }
            )
            merged.append("\n" + marker["marker_text"] + "\n")
            merged.append(next_seg)
            continue

        logs.append(
            {
                "ã¤ãªãç›®ç•ªå·": idx,
                "ãƒ•ã‚¡ã‚¤ãƒ«å": marker.get("file_name", ""),
                "æ¤œå‡ºçµæœ": "æ¤œå‡º",
                "é–‹å§‹ä½ç½®": start_pos,
                "ä¸€è‡´æ–‡å­—æ•°": size,
                "head_phrase": base_head,
                "shifted_phrases": shifted_heads,
                "matched_phrase": matched_phrase,
            }
        )

        new_prev = prev_seg[:start_pos] + "\n" + BEGIN_TAG + "\n" + prev_seg[start_pos:]
        merged[-1] = new_prev

        merged.append("\n" + marker["marker_text"] + "\n")
        merged.append(next_seg)

    return "".join(merged), logs


# ============================================================
# Streamlit UIï¼ˆstorageå¯¾å¿œï¼‰
# ============================================================
st.set_page_config(
    page_title="ğŸ“ é‡è¤‡ç®‡æ‰€æ¤œå‡ºï¼ˆstorageå¯¾å¿œï¼‰",
    page_icon="ğŸ“",
    layout="wide",
)

sub = require_login(st)
if not sub:
    st.stop()
left, right = st.columns([2, 1])
with left:
    st.title("ğŸ“ é‡è¤‡ç®‡æ‰€æ¤œå‡ºï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œï¼‰")
with right:
    st.success(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³ä¸­: **{sub}**")
current_user=sub


st.markdown(
    """
- **ãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆ**ï¼ˆCookie/JWTï¼‰ã€‚
- Storages å†…ã® **è©±è€…åˆ†é›¢å¾Œã®é€£çµï¼ˆtranscript_speaker_separated_combined/*.txtï¼‰** ã‚’é¸æŠã—ã¦å‡¦ç†ã—ã¾ã™ã€‚
- å„ã¤ãªãç›®ã§ã€å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’ã‚­ãƒ¼ã«å‰åŠæœ«å°¾ã‹ã‚‰ä¸€è‡´ä½ç½®ã‚’æ¢ã—ã€
  **å‰åŠå´ã«ã ã‘** `-----ã“ã“ã‹ã‚‰é‡è¤‡-----` ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚
"""
)

user_dir = _sanitize_username_for_path(str(current_user))

# =========================
# Sidebar params
# =========================
with st.sidebar:
    st.header("æ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    min_match_size = st.slider(
        "ä¸€è‡´ã¨ã¿ãªã™æœ€ä½æ–‡å­—æ•°ï¼ˆMIN_MATCH_SIZEï¼‰",
        min_value=5,
        max_value=40,
        value=DEFAULT_MIN_MATCH_SIZE,
        step=1,
        help="å°ã•ã„ã»ã©æ¤œå‡ºãŒã‚†ã‚‹ããªã‚Šã¾ã™ã€‚",
    )

    autojunk_option = st.radio(
        "autojunkï¼ˆSequenceMatcher è‡ªå‹•ã‚¸ãƒ£ãƒ³ã‚¯åˆ¤å®šï¼‰",
        options=["ONï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰", "OFFï¼ˆçŸ­ã„æ–‡ã§ã‚‚ç²¾ç¢ºã«ï¼‰"],
        index=0,
        help="OFF ã«ã™ã‚‹ã¨çŸ­ã„ä¸€è‡´ã‚’æ‹¾ã„ã‚„ã™ããªã‚Šã¾ã™ï¼ˆå°‘ã—é…ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚",
    )
    use_autojunk = autojunk_option.startswith("ON")

    st.divider()
    st.caption("ä¿å­˜å…ˆã¯ã€é¸æŠã‚¸ãƒ§ãƒ–ã® transcript/ ã§ã™ã€‚")

# =========================
# combined txt selection
# =========================
items = list_combined_texts(user_dir)
if not items:
    st.info(
        "Storages ã«é€£çµæ–‡å­—èµ·ã“ã—ï¼ˆtranscripts_combined_*.txtï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
        "å…ˆã«ã€Œæ–‡å­—èµ·ã“ã—ï¼ˆstorageå¯¾å¿œï¼‰ã€ã§ transcript ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
    )
    st.stop()

labels = [it.label for it in items]
picked = st.radio("å‡¦ç†å¯¾è±¡ï¼ˆcombined txtï¼‰", options=labels, index=0)
it = items[labels.index(picked)]

st.caption(f"é¸æŠãƒ•ã‚¡ã‚¤ãƒ«: {it.path}")

with st.expander("ğŸ“Œ é¸æŠã‚¸ãƒ§ãƒ–æƒ…å ±", expanded=False):
    st.write(
        {
            "job_dir": str(it.job_dir),
            "transcript_dir": str(it.transcript_dir),
            "job_id": it.job_id,
            "date": it.date,
            "created_at": it.created_at,
        }
    )

run = st.button("â–¶ï¸ é‡è¤‡ç®‡æ‰€æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹", type="primary")

if run:
    # èª­ã‚€
    try:
        text = it.path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        text = it.path.read_text(encoding="cp932", errors="replace")

    # å®Ÿè¡Œ
    with st.spinner("é‡è¤‡ç®‡æ‰€ã‚’æ¤œå‡ºã—ã¦ã„ã¾ã™â€¦"):
        merged, logs = build_merged_text(text, min_match_size, use_autojunk)

    # ä¿å­˜ï¼ˆåŒã˜ transcript/ï¼‰
    ts_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
    # â˜… è¿½åŠ ï¼šä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ï¼ˆtranscript_markedï¼‰
    marked_dir = it.job_dir / "transcript_marked"
    safe_mkdir(marked_dir)

    # ==============================
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    # ==============================
    out_txt = marked_dir / f"{it.path.stem}_marked.txt"
    out_log = marked_dir / f"{it.path.stem}_detect_log.json"

    write_text(out_txt, merged)
    write_json(
        out_log,
        {
            "input": str(it.path),
            "output_text": str(out_txt),
            "output_log": str(out_log),
            "params": {
                "OVERLAP_CHARS": OVERLAP_CHARS,
                "HEAD_CHARS": HEAD_CHARS,
                "HEAD_SENTENCES": HEAD_SENTENCES,
                "HEAD_SHIFT_TRIES": HEAD_SHIFT_TRIES,
                "min_match_size": int(min_match_size),
                "autojunk": bool(use_autojunk),
                "BEGIN_TAG": BEGIN_TAG,
            },
            "counts": {"markers": len(logs), "has_markers": bool(logs)},
            "logs": logs,
            "created_at": datetime.now().isoformat(timespec="seconds"),
            "user": str(current_user),
            "job_dir": str(it.job_dir),
        },
    )

    # jobã® process.log ã«ã‚‚è»½ãæ›¸ã
    log_path = it.job_dir / "logs" / "process.log"
    append_log(log_path, "OVERLAP DETECT START")
    append_log(log_path, f"input={it.path.name}")
    append_log(log_path, f"output={out_txt.name}")
    append_log(log_path, f"log={out_log.name}")
    append_log(log_path, f"min_match_size={min_match_size} autojunk={use_autojunk}")
    append_log(log_path, "OVERLAP DETECT DONE")

    # è¡¨ç¤º
    st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ã—ã¾ã—ãŸï¼‰ã€‚")

    st.markdown("### ğŸ’¾ ä¿å­˜å…ˆï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰")
    st.write({"merged_txt": str(out_txt), "log_json": str(out_log)})

    st.markdown("### ğŸ” ã¤ãªãç›®ã”ã¨ã®æ¤œå‡ºãƒ­ã‚°ï¼ˆæ¦‚è¦ï¼‰")
    if logs:
        st.table(
            [
                {
                    "ã¤ãªãç›®ç•ªå·": item["ã¤ãªãç›®ç•ªå·"],
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": item["ãƒ•ã‚¡ã‚¤ãƒ«å"],
                    "æ¤œå‡ºçµæœ": item["æ¤œå‡ºçµæœ"],
                    "é–‹å§‹ä½ç½®": item["é–‹å§‹ä½ç½®"],
                    "ä¸€è‡´æ–‡å­—æ•°": item["ä¸€è‡´æ–‡å­—æ•°"],
                }
                for item in logs
            ]
        )
    else:
        st.info("ã¤ãªãç›®ãƒãƒ¼ã‚«ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ­ã‚°ã¯ç©ºï¼‰ã€‚")

    with st.expander("ğŸ§© head_phrase / matched_phrase è©³ç´°", expanded=False):
        if not logs:
            st.info("ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            for item in logs:
                st.markdown(f"#### ã¤ãªãç›® {item['ã¤ãªãç›®ç•ªå·']} â€” {item['æ¤œå‡ºçµæœ']}")
                st.markdown("**head_phrase**")
                st.code(item.get("head_phrase") or "ï¼ˆç©ºï¼‰")
                st.markdown("**shifted_phrases**")
                shifted = item.get("shifted_phrases") or []
                if shifted:
                    for s in shifted:
                        st.code(s)
                else:
                    st.write("ï¼ˆãªã—ï¼‰")
                st.markdown("**matched_phrase**")
                st.code(item.get("matched_phrase") or "ï¼ˆãƒãƒƒãƒãªã—ï¼‰")
                st.markdown("---")

    with st.expander("ğŸ“˜ çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆmergedï¼‰", expanded=False):
        st.text(merged)

    st.download_button(
        "ğŸ“¥ çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.txt)",
        data=merged.encode("utf-8"),
        file_name=f"{it.path.stem}_é‡è¤‡ç®‡æ‰€æ¤œå‡º_{ts_tag}.txt",
        mime="text/plain",
    )
