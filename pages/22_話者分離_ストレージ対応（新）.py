# -*- coding: utf-8 -*-
# pages/22_è©±è€…åˆ†é›¢_storageå¯¾å¿œï¼ˆæ–°ï¼‰.py
# ------------------------------------------------------------
# ğŸ™ï¸ è©±è€…åˆ†é›¢ãƒ»æ•´å½¢ï¼ˆè­°äº‹éŒ²ã®å‰å‡¦ç†ï¼‰storageå¯¾å¿œï¼ˆãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆï¼‰
# - æœ€åˆã« job_xxxx ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’ radio ã§é¸æŠï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã‚’å…¨åˆ—æŒ™ï¼‰
# - é¸æŠã—ãŸ job ã® transcript/*.txt ã‚’é †ç•ªã«ã™ã¹ã¦è©±è€…åˆ†é›¢ï¼ˆå˜ç™ºãªã—ï¼‰
# - è©±è€…åˆ†é›¢çµæœã¯ transcript_speaker_separated/ ã«å€‹åˆ¥ä¿å­˜
# - ãã®å¾Œã€æŒ‡å®šã®ã€Œã¤ãªãã€è¡Œã‚’æŒŸã‚“ã§å…¨é€£çµã—
#   transcript_speaker_separated_combined/ ã«ä¿å­˜
# - å·¨å¤§ãƒ†ã‚­ã‚¹ãƒˆã‚’ AI ã«ç›´æ¥æŠ•å…¥ã—ãªã„ï¼ˆåˆ†å‰²â†’è©±è€…åˆ†é›¢â†’é€£çµï¼‰
#
# â€» common_lib ã¯æ”¹å¤‰ã—ãªã„
# â€» use_container_width ã¯ä½¿ã‚ãªã„ï¼ˆæ–¹é‡ï¼‰
# ------------------------------------------------------------

from __future__ import annotations

import json
import time
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Optional, List

import streamlit as st
from openai import OpenAI
import google.generativeai as genai

# ==== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
from lib.costs import estimate_chat_cost_usd
from lib.tokens import extract_tokens_from_response
from lib.prompts import SPEAKER_PREP, get_group, build_prompt
from lib.prompts import (
    SPEAKER_MANDATORY,
    SPEAKER_MANDATORY_LIGHT,
    SPEAKER_MANDATORY_LIGHTER,
)
from config.config import (
    DEFAULT_USDJPY,
    get_gemini_api_key,
    has_gemini_api_key,
    estimate_tokens_from_text,
    estimate_gemini_cost_usd,
)
from ui.style import disable_heading_anchors
from lib.explanation import render_speaker_prep_expander

# ============================================================
# sys.path èª¿æ•´ï¼ˆcommon_lib ã‚’ import ã§ãã‚‹ã‚ˆã†ã«ï¼‰
# ============================================================
import sys

_THIS = Path(__file__).resolve()
PROJECTS_ROOT = _THIS.parents[3]

if str(PROJECTS_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECTS_ROOT))

from common_lib.storage.external_ssd_root import resolve_storage_subdir_root
from common_lib.auth.auth_helpers import require_login

STORAGE_ROOT = resolve_storage_subdir_root(
    PROJECTS_ROOT,
    subdir="Storages",
)

# ============================================================
# utils
# ============================================================
def safe_mkdir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def write_text(p: Path, s: str) -> None:
    safe_mkdir(p.parent)
    p.write_text(s, encoding="utf-8")


def write_json(p: Path, obj: Any) -> None:
    safe_mkdir(p.parent)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def append_log(log_path: Path, msg: str) -> None:
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    safe_mkdir(log_path.parent)
    with log_path.open("a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")


def _sanitize_username_for_path(username: str) -> str:
    u = (username or "").strip()
    if not u:
        return "anonymous"
    u = re.sub(r"[^0-9A-Za-z_-]+", "_", u).strip("_")
    return u or "anonymous"


def _human_dt(s: str | None) -> str:
    if not s:
        return "â€”"
    try:
        return s.replace("T", " ").replace("+00:00", "Z")
    except Exception:
        return s


def _read_job_json(job_dir: Path) -> dict:
    p = job_dir / "job.json"
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}


# ============================================================
# ã“ã“ã‚’è¿½åŠ ï¼šoriginal ã‹ã‚‰ base / original ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ‹¾ã†
# ï¼ˆç„¡ã‘ã‚Œã° "none"ï¼‰
# ============================================================
def get_base_from_original(job_dir: Path) -> tuple[str, str]:
    """
    job_xxxx/original/* ã®å…ˆé ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤ºç”¨ã«æ‹¾ã†
    - ç„¡ã‘ã‚Œã° ("none", "none")
    """
    original_dir = job_dir / "original"
    if not original_dir.exists():
        return "none", "none"

    files = list(original_dir.iterdir())
    if not files:
        return "none", "none"

    p = files[0]
    return p.stem, p.name


# ============================================================
# job listingï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã‚’ã™ã¹ã¦åˆ—æŒ™ï¼‰
# ============================================================
@dataclass
class JobItem:
    label: str
    job_dir: Path
    date: str
    job_id: str
    created_at: Optional[str]

# ============================================================
# ã“ã“ã‚’ç½®ãæ›ãˆï¼šlist_all_jobs() ã‚’ä¸¸ã”ã¨å·®ã—æ›¿ãˆ
# ï¼ˆlabel ã« original å + base åã‚’å«ã‚ã‚‹ï¼‰
# ============================================================
def list_all_jobs(user_dir: str) -> list[JobItem]:
    base = STORAGE_ROOT / user_dir / "minutes_app"
    if not base.exists():
        return []

    items: list[JobItem] = []

    for day_dir in sorted(base.glob("*"), reverse=True):
        if not day_dir.is_dir():
            continue

        for job_dir in sorted(day_dir.glob("job_*"), reverse=True):
            if not job_dir.is_dir():
                continue

            # transcript ãŒã‚ã‚‹ job ã®ã¿
            if not (job_dir / "transcript").exists():
                continue

            meta = _read_job_json(job_dir)
            job_id = str(meta.get("job_id") or job_dir.name)
            date = str(meta.get("date") or day_dir.name)
            created_at = meta.get("created_at")

            base_stem, original_name = get_base_from_original(job_dir)

            # radio ã¯æ”¹è¡Œè¡¨ç¤ºã•ã‚Œã‚‹ã®ã§ã€è¦‹ã‚„ã™ãè¤‡æ•°è¡Œã«ã™ã‚‹
            label = (
                f"{date} / {job_dir.name}\n"
                f"  â”” original: {original_name}\n"
                f"  â”” base: {base_stem} / created={_human_dt(created_at)}"
            )

            items.append(
                JobItem(
                    label=label,
                    job_dir=job_dir,
                    date=date,
                    job_id=job_id,
                    created_at=created_at,
                )
            )

    return items


# ============================================================
# transcript/*.txt ã‚’é †ç•ªã«å‡¦ç†
# ============================================================
_PART_RE = re.compile(r"_part(\d+)_", re.IGNORECASE)

def _sort_key_transcript(p: Path) -> tuple[int, str]:
    m = _PART_RE.search(p.name)
    if m:
        return (int(m.group(1)), p.name.lower())
    return (10**9, p.name.lower())


def list_transcript_txts(job_dir: Path) -> list[Path]:
    transcript_dir = job_dir / "transcript"
    if not transcript_dir.exists():
        return []

    files: list[Path] = []
    for p in transcript_dir.glob("*.txt"):
        n = p.name.lower()
        if "speaker" in n:
            continue
        if "marked" in n:
            continue
        files.append(p)

    return sorted(files, key=_sort_key_transcript)


def make_connector_line(prev_name: str) -> str:
    return f"----- ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆ{prev_name} ã¨æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®é–“ï¼‰-----"

# ============================================================
# AI å®Ÿè¡Œï¼ˆå˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
# ============================================================
def run_speaker_prep_one(prompt: str, model: str, client, usd_jpy: float):
    t0 = time.perf_counter()

    if model.startswith("gemini"):
        gem = genai.GenerativeModel(model)
        resp = gem.generate_content(prompt)
        text = getattr(resp, "text", "") or ""
        elapsed = time.perf_counter() - t0

        out_tok = estimate_tokens_from_text(text)
        in_tok = estimate_tokens_from_text(prompt)
        usd = estimate_gemini_cost_usd(
            model=model, input_tokens=in_tok, output_tokens=out_tok
        )
        jpy = (usd * usd_jpy) if usd is not None else None
        return text, elapsed, in_tok, out_tok, usd, jpy

    # OpenAI
    if client is None:
        raise RuntimeError("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")

    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_completion_tokens=100000,
    )
    elapsed = time.perf_counter() - t0
    text = resp.choices[0].message.content or ""

    in_tok, out_tok, _ = extract_tokens_from_response(resp)
    usd = estimate_chat_cost_usd(model, in_tok, out_tok)
    jpy = (usd * usd_jpy) if usd is not None else None
    return text, elapsed, in_tok, out_tok, usd, jpy

# ============================================================
# UI è¨­å®š
# ============================================================
st.set_page_config(
    page_title="â‘¢ è©±è€…åˆ†é›¢ãƒ»æ•´å½¢ï¼ˆstorageå¯¾å¿œï¼‰",
    page_icon="ğŸ™ï¸",
    layout="wide",
)
disable_heading_anchors()

sub = require_login(st)
if not sub:
    st.stop()
left, right = st.columns([2, 1])
with left:
    st.title("ğŸ™ï¸ è©±è€…åˆ†é›¢ãƒ»æ•´å½¢ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¯¾å¿œï¼‰")
with right:
    st.success(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³ä¸­: **{sub}**")
current_user=sub

user_dir = _sanitize_username_for_path(str(current_user))

#st.title("ğŸ™ï¸ è©±è€…åˆ†é›¢ãƒ»æ•´å½¢ï¼ˆstorageå¯¾å¿œï¼‰")

render_speaker_prep_expander()

st.markdown(
    """
- **ãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆ**
- æœ€åˆã« **job_xxxx ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’ radio ã§é¸æŠ**
- é¸æŠã—ãŸ job ã® **transcript/*.txt ã‚’é †ç•ªã«ã™ã¹ã¦è©±è€…åˆ†é›¢**
- è©±è€…åˆ†é›¢å¾Œã« **ã¤ãªãè¡Œã‚’æŒŸã‚“ã§é€£çµ**
"""
)

# ============================================================
# ãƒ­ã‚°ã‚¤ãƒ³
# ============================================================


# ============================================================
# OpenAI / Gemini init
# ============================================================
OPENAI_API_KEY = st.secrets.get("openai", {}).get("api_key") or st.secrets.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

GEMINI_ENABLED = has_gemini_api_key()
if GEMINI_ENABLED:
    genai.configure(api_key=get_gemini_api_key())

# ============================================================
# Sidebarï¼ˆãƒ¢ãƒ‡ãƒ«/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ/é€šè²¨ï¼‰
# ============================================================
with st.sidebar:
    st.subheader("ãƒ¢ãƒ‡ãƒ«è¨­å®š")

    MODEL_OPTIONS = [
        "gpt-5-mini",
        "gpt-5-nano",
        "gemini-2.0-flash",
    ]

    st.session_state.setdefault("speaker_model", "gpt-5-mini")
    model = st.radio("ãƒ¢ãƒ‡ãƒ«", MODEL_OPTIONS, key="speaker_model")

    if model.startswith("gemini") and not GEMINI_ENABLED:
        st.warning("Gemini API Key ãŒæœªè¨­å®šã§ã™ã€‚")
        st.stop()

    st.subheader("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š")

    PROMPT_LEVEL_OPTIONS = [
        "æ¨™æº–ï¼ˆç²¾åº¦å„ªå…ˆï¼‰",
        "è»½é‡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä½æ¸›ï¼‰",
        "è¶…è»½é‡ï¼ˆæœ€å°è² è·ï¼‰",
    ]
    st.session_state.setdefault("speaker_prompt_level", PROMPT_LEVEL_OPTIONS[0])
    prompt_level = st.radio(
        "è©±è€…åˆ†é›¢ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        PROMPT_LEVEL_OPTIONS,
        key="speaker_prompt_level",
    )

    st.subheader("é€šè²¨æ›ç®—")
    usd_jpy = st.number_input(
        "USD/JPY",
        min_value=50.0,
        max_value=500.0,
        value=float(DEFAULT_USDJPY),
        step=0.5,
    )

# ============================================================
# ãƒ¡ã‚¤ãƒ³ UI
# ============================================================
left, right = st.columns([1, 1], gap="large")

# ---- å³ï¼šjob é¸æŠ ----
with right:
    st.subheader("â‘  job ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼é¸æŠ")

    jobs = list_all_jobs(user_dir)
    if not jobs:
        st.info("minutes_app ã® job ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    labels = [j.label for j in jobs]
    picked = st.radio("å‡¦ç†å¯¾è±¡ã® job", options=labels, index=0)
    job = jobs[labels.index(picked)]
    job_dir = job.job_dir

    st.caption(f"job_dir: {job_dir}")

    transcript_files = list_transcript_txts(job_dir)
    if not transcript_files:
        st.error("transcript/*.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    st.markdown("### å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‡¦ç†é †ï¼‰")
    st.write([p.name for p in transcript_files])

    st.session_state["speaker_job_dir"] = str(job_dir)
    st.session_state["speaker_transcript_files"] = [str(p) for p in transcript_files]


# ---- å·¦ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ & å®Ÿè¡Œ ----
with left:
    st.subheader("â‘¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")

    group = get_group(SPEAKER_PREP)

    _level = st.session_state.get("speaker_prompt_level")
    if _level == "æ¨™æº–ï¼ˆç²¾åº¦å„ªå…ˆï¼‰":
        mandatory_default = SPEAKER_MANDATORY
    elif _level == "è»½é‡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä½æ¸›ï¼‰":
        mandatory_default = SPEAKER_MANDATORY_LIGHT
    else:
        mandatory_default = SPEAKER_MANDATORY_LIGHTER

    # ============================================================
    # â˜… è¿½åŠ ï¼šprompt level ã®å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¦ mandatory_prompt ã‚’è‡ªå‹•æ›´æ–°
    #   - level ãŒå¤‰ã‚ã£ãŸã¨ãã ã‘ mandatory_prompt ã‚’å·®ã—æ›¿ãˆã‚‹
    #   - åˆå›ã¯ setdefault ã§å…¥ã‚Œã‚‹
    # ============================================================
    prev_level = st.session_state.get("_speaker_prompt_level_prev")

    if prev_level is None:
        st.session_state.setdefault("mandatory_prompt", mandatory_default)
        st.session_state["_speaker_prompt_level_prev"] = _level
    else:
        if prev_level != _level:
            st.session_state["mandatory_prompt"] = mandatory_default
            st.session_state["_speaker_prompt_level_prev"] = _level
        else:
            st.session_state.setdefault("mandatory_prompt", mandatory_default)

    st.text_area("å¿…é ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", height=220, key="mandatory_prompt")

    st.session_state.setdefault(
        "preset_label", group.label_for_key(group.default_preset_key)
    )
    st.session_state.setdefault(
        "preset_text", group.body_for_label(st.session_state["preset_label"])
    )
    st.session_state.setdefault("extra_text", "")

    def _on_change_preset():
        st.session_state["preset_text"] = group.body_for_label(
            st.session_state["preset_label"]
        )

    st.selectbox(
        "è¿½è¨˜ãƒ—ãƒªã‚»ãƒƒãƒˆ",
        options=group.preset_labels(),
        key="preset_label",
        on_change=_on_change_preset,
    )

    st.text_area("ãƒ—ãƒªã‚»ãƒƒãƒˆæœ¬æ–‡", height=120, key="preset_text")
    st.text_area("è¿½åŠ æŒ‡ç¤ºï¼ˆä»»æ„ï¼‰", height=80, key="extra_text")

    batch_btn = st.button(
        "â‘¢ transcript ã‚’é †ç•ªã«è©±è€…åˆ†é›¢ï¼ˆä¿å­˜ï¼‹é€£çµï¼‰", type="primary"
    )


# ============================================================
# å®Ÿè¡Œï¼ˆãƒãƒƒãƒã®ã¿ï¼‰
# ============================================================
if batch_btn:
    job_dir = Path(st.session_state["speaker_job_dir"])
    transcript_files = [Path(p) for p in st.session_state["speaker_transcript_files"]]

    speaker_sep_dir = job_dir / "transcript_speaker_separated"
    speaker_comb_dir = job_dir / "transcript_speaker_separated_combined"
    logs_dir = job_dir / "logs"
    safe_mkdir(speaker_sep_dir)
    safe_mkdir(speaker_comb_dir)
    safe_mkdir(logs_dir)

    ts_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
    combined_path = speaker_comb_dir / f"transcript_speaker_separated_combined_{ts_tag}.txt"

    combined_chunks: List[str] = []
    total_in = total_out = 0
    total_usd = 0.0

    log_path = logs_dir / "process.log"
    append_log(log_path, "SPEAKER PREP BATCH START")
    append_log(log_path, f"job_dir={job_dir}")
    append_log(log_path, f"count={len(transcript_files)} model={model}")

    prog = st.progress(0)
    status = st.empty()

    for i, src_path in enumerate(transcript_files, start=1):
        status.write(f"{i}/{len(transcript_files)} å‡¦ç†ä¸­: {src_path.name}")
        src = src_path.read_text(encoding="utf-8", errors="ignore").strip()
        if not src:
            continue

        prompt = build_prompt(
            st.session_state["mandatory_prompt"],
            st.session_state["preset_text"],
            st.session_state["extra_text"],
            src,
        )

        text, elapsed, in_tok, out_tok, usd, jpy = run_speaker_prep_one(
            prompt, model, client, usd_jpy
        )

        out_path = speaker_sep_dir / f"{i:03d}_{src_path.stem}_speaker.txt"
        write_text(out_path, text or "")

        combined_chunks.append(text or "")
        if i < len(transcript_files):
            combined_chunks.append(make_connector_line(src_path.name))

        total_in += int(in_tok)
        total_out += int(out_tok)
        if usd is not None:
            total_usd += float(usd)

        append_log(
            log_path,
            f"DONE {src_path.name} -> {out_path.name} in={in_tok} out={out_tok} usd={usd}",
        )

        prog.progress(int(i / len(transcript_files) * 100))

    combined_text = "\n\n".join(combined_chunks)
    write_text(combined_path, combined_text)

    st.success(f"âœ… ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ã—ã¾ã—ãŸ: {combined_path.name}")
    st.caption(f"ä¿å­˜å…ˆ: {combined_path}")


    append_log(log_path, f"COMBINED -> {combined_path.name}")
    append_log(log_path, "SPEAKER PREP BATCH DONE")

    st.success("å®Œäº†ã—ã¾ã—ãŸã€‚")
    st.write(
        {
            "speaker_separated_dir": str(speaker_sep_dir),
            "speaker_combined_path": str(combined_path),
            "total_tokens": {"input": total_in, "output": total_out},
            "total_usd_est": total_usd,
            "total_jpy_est": total_usd * usd_jpy,
        }
    )

    st.download_button(
        "é€£çµçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ.txtï¼‰",
        data=combined_text.encode("utf-8"),
        file_name=combined_path.name,
        mime="text/plain",
    )
