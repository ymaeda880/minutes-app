# -*- coding: utf-8 -*-
# pages/03_é‡è¤‡ç®‡æ‰€æ¤œå‡º2.py
#
# å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’ã‚­ãƒ¼ã«ã€
# å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ï¼ˆæ–‡å­—æ•°ï¼‰ã€ã®ä¸­ã‹ã‚‰
# ã‚‚ã£ã¨ã‚‚ã‚ˆãä¸€è‡´ã™ã‚‹ä½ç½®ã‚’æ¢ã—ã€
# å‰åŠå´ã«ã ã‘ã€Œ-----ã“ã“ã‹ã‚‰é‡è¤‡-----ã€ã‚’æŒ¿å…¥ã™ã‚‹ã€‚
#
# å¾ŒåŠå´ã«ã¯ã€Œã“ã“ã¾ã§ãŒé‡è¤‡éƒ¨åˆ†ã€ã¯å…¥ã‚Œãªã„ã€‚

from __future__ import annotations

import re
from difflib import SequenceMatcher
from typing import List, Dict, Tuple, Any

import streamlit as st

# ============================================================
# è¨­å®šå€¤
# ============================================================

# ã€Œå‰å¾Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ¯”è¼ƒç¯„å›²ã€ï¼ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—é•·ï¼ˆå‰åŠã®æœ€å¾Œã®ä½•æ–‡å­—ã‚’è¦‹ã‚‹ã‹ï¼‰
OVERLAP_CHARS = 700  # ãŠãŠã‚ˆã2åˆ†ç›¸å½“ã®ç›®å®‰

# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã¨ã—ã¦ä½¿ã†æœ€å¤§æ–‡å­—æ•°
HEAD_CHARS = 400

# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã¨ã—ã¦åŒºåˆ‡ã‚‹æœ€å¤§æ–‡æ•°
HEAD_SENTENCES = 3

# ä¸€è‡´ã¨ã¿ãªã™æœ€ä½æ–‡å­—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰â€»ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¤‰æ›´å¯èƒ½
DEFAULT_MIN_MATCH_SIZE = 20

# ã‚­ãƒ¼ã«ãªã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã€Œå…ˆé ­1æ–‡å­—ãšã¤ãšã‚‰ã—ã¦ã€è©¦ã™æœ€å¤§å›æ•°
HEAD_SHIFT_TRIES = 3  # ä¾‹ï¼šä»Šæ—¥ã¯ä¼šè­°ã‚’â†’æ—¥ã¯ä¼šè­°ã‚’â†’ã¯ä¼šè­°ã‚’â†’ä¼šè­°ã‚’â€¦ ã®ã‚¤ãƒ¡ãƒ¼ã‚¸

# å¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼è¡Œ
MARKER_PATTERN = re.compile(
    r"^-{3,}\s*ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆ(.*?)ï¼‰.*$",
    re.MULTILINE,
)

BEGIN_TAG = "-----ã“ã“ã‹ã‚‰é‡è¤‡-----"


# ============================================================
# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²
# ============================================================

def split_by_markers(text: str) -> Tuple[List[str], List[Dict[str, str]]]:
    """
    å…¨æ–‡ã‹ã‚‰ã€Œã¤ãªãç›®ãƒãƒ¼ã‚«ãƒ¼è¡Œã€ã§åˆ†å‰²ã—ã¦
    segments ã¨ markers ã‚’è¿”ã™ã€‚

    [seg0][marker0][seg1][marker1][seg2]...
    â†’ segments = [seg0, seg1, seg2, ...]
      markers  = [marker0, marker1, ...]
    """
    segments: List[str] = []
    markers: List[Dict[str, str]] = []

    prev_end = 0

    for m in MARKER_PATTERN.finditer(text):
        seg = text[prev_end:m.start()]
        segments.append(seg)

        markers.append(
            {
                "file_name": m.group(1),
                "marker_text": m.group(0),
            }
        )
        prev_end = m.end()

    segments.append(text[prev_end:])
    return segments, markers


# ============================================================
# ã€Œå¾ŒåŠã®æœ€åˆã®æ•°è¡Œã€ã‚’æŠœãå‡ºã™
# ============================================================

def extract_head_phrase(next_seg: str) -> str:
    """
    å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’å–ã‚Šå‡ºã™ã€‚

    - å…ˆé ­ HEAD_CHARS æ–‡å­—ã‚’å¯¾è±¡
    - ãã®ä¸­ã§ã€Œã€‚ã€ã€Œï¼Ÿã€ã€Œï¼ã€ã€Œæ”¹è¡Œã€ã‚’æ–‡ã®åŒºåˆ‡ã‚Šã¨ã¿ãªã—ã€
      HEAD_SENTENCES æ–‡ã¾ã§å«ã‚ãŸéƒ¨åˆ†ã‚’ head_phrase ã¨ã—ã¦è¿”ã™ã€‚
    """
    if not next_seg:
        return ""

    s = next_seg[:HEAD_CHARS]
    count = 0
    end = len(s)

    for i, ch in enumerate(s):
        if ch in "ã€‚ï¼Ÿï¼\n":
            count += 1
            if count >= HEAD_SENTENCES:
                end = i + 1  # åŒºåˆ‡ã‚Šè¨˜å·ã‚‚å«ã‚ã‚‹
                break

    return s[:end]


# ============================================================
# é‡è¤‡é–‹å§‹ä½ç½®ã®æ¢ç´¢ï¼ˆå‰åŠå´ã®ã¿ï¼‰
# ============================================================

# æ­£è¦åŒ–ï¼šå¥èª­ç‚¹ãƒ»æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ï¼ˆå¿…è¦ãªã‚‰ã•ã‚‰ã«è¿½åŠ å¯ï¼‰
def normalize_text(s: str) -> str:
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ â†’ ç©º
    s = s.replace("ã€€", "")
    # æ”¹è¡Œ â†’ ç©º
    s = s.replace("\n", "")
    # å¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å»
    s = re.sub(r"[ã€ã€‚ï¼ï¼Ÿ,.!?]", "", s)
    # ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹é™¤å»
    s = s.replace(" ", "")
    return s


def _match_with_phrase(
    prev_seg: str,
    phrase: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[int, int]:
    """
    1ã¤ã®ã€Œã‚­ãƒ¼ã«ãªã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆphraseï¼‰ã€ã«å¯¾ã—ã¦ã€
    å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆ prev_seg ã®æœ«å°¾ OVERLAP_CHARS ã¨ãƒãƒƒãƒãƒ³ã‚°ã‚’è¡Œã„ã€
    ä¸€è‡´é–‹å§‹ä½ç½®ã¨ä¸€è‡´é•·ã‚’è¿”ã™ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° -1, 0ï¼‰ã€‚
    """
    if not prev_seg or not phrase:
        return -1, 0

    if len(phrase) < min_match_size:
        return -1, 0

    # å‰åŠå´ã®æœ«å°¾
    prev_tail = prev_seg[-OVERLAP_CHARS:]

    # æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    norm_head = normalize_text(phrase)
    norm_prev = normalize_text(prev_tail)

    if len(norm_head) < min_match_size:
        return -1, 0

    # é¡ä¼¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºï¼ˆâ˜… autojunk ã‚’ UI ã‹ã‚‰åˆ¶å¾¡ï¼‰
    sm = SequenceMatcher(None, norm_head, norm_prev, autojunk=use_autojunk)
    blocks = sm.get_matching_blocks()

    # å€™è£œæŠ½å‡º
    cand = [(b.a, b.b, b.size) for b in blocks if b.size >= min_match_size]
    if not cand:
        return -1, 0

    # å„ªå…ˆé †ä½ï¼š
    #   head_phrase å´ã®é–‹å§‹ãŒå…ˆ â†’ prev_tail å´ã®é–‹å§‹ãŒå…ˆ â†’ size ãŒå¤§ãã„
    a, b, size = sorted(cand, key=lambda t: (t[0], t[1], -t[2]))[0]

    # ======== å…ƒãƒ†ã‚­ã‚¹ãƒˆã® index ã«æˆ»ã™ãŸã‚ã®å‡¦ç† ========

    # æ­£è¦åŒ–å‰ã® phrase ã¨ prev_tail ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œã‚‹
    def build_index_map(raw: str, norm: str):
        """
        æ­£è¦åŒ–å‰ raw ã®å„æ–‡å­—ãŒã€æ­£è¦åŒ–å¾Œ norm ã®
        ã©ã® index ã«å¯¾å¿œã™ã‚‹ã‹ã‚’è¿”ã™ map
        """
        mapping = []
        j = 0
        for i, ch in enumerate(raw):
            # æ­£è¦åŒ–ã§æ¶ˆãˆã‚‹æ–‡å­—ã¯ mapping ã«å…¥ã‚Œãªã„
            ch_norm = normalize_text(ch)
            if ch_norm == "":
                continue
            if j < len(norm):
                mapping.append((j, i))
                j += 1
        return mapping

    head_map = build_index_map(phrase, norm_head)
    prev_map = build_index_map(prev_tail, norm_prev)

    # a, b ã¯æ­£è¦åŒ–å¾Œã® index ãªã®ã§ raw å´ index ã«é€†å¤‰æ›ã™ã‚‹
    def mapped_index(mapping, idx_norm):
        # idx_norm ã«æœ€ã‚‚è¿‘ã„ mapping ã® raw index ã‚’è¿”ã™
        candidates = [raw_idx for norm_idx, raw_idx in mapping if norm_idx == idx_norm]
        if candidates:
            return candidates[0]
        # ç›´æ¥ä¸€è‡´ãŒãªã‘ã‚Œã°è¿‘ã„ã‚‚ã®ã‚’æ¢ã™
        nearest = None
        best_dist = 10**9
        for norm_idx, raw_idx in mapping:
            d = abs(norm_idx - idx_norm)
            if d < best_dist:
                best_dist = d
                nearest = raw_idx
        return nearest

    # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®é–‹å§‹ä½ç½®
    head_raw_start = mapped_index(head_map, a)
    prev_raw_start = mapped_index(prev_map, b)

    # ã€Œhead ã®å…ˆé ­ã¨ align ã™ã‚‹ã€å‰æã§è£œæ­£
    start_in_tail = max(0, prev_raw_start - head_raw_start)

    global_prev_start = len(prev_seg) - len(prev_tail) + start_in_tail

    return global_prev_start, size


def find_overlap_start(
    prev_seg: str,
    next_seg: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[int, int, str, List[str], str]:
    """
    1. å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ head_phraseï¼ˆæœ€åˆã®æ•°è¡Œï¼‰ã‚’ä½œã‚‹ã€‚
    2. strip + rstrip ã§æœ«å°¾ã®å¥èª­ç‚¹ã‚’è½ã¨ã—ã¦ base_head ã«ã™ã‚‹ã€‚
    3. base_head ã¨ã€ãã“ã‹ã‚‰ 1æ–‡å­—ãšã¤å…ˆé ­ã‚’å‰Šã£ãŸãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é †ã«è©¦ã™ã€‚
    4. ã©ã‚Œã‹ã§è¦‹ã¤ã‹ã‚Œã°ã€ãã®ä½ç½®ã¨é•·ã•ãƒ»base_headãƒ»shiftedãƒ»matched_phrase ã‚’è¿”ã™ã€‚
    """

    if not prev_seg or not next_seg:
        return -1, 0, "", [], ""

    # ã¾ãšã€Œç”Ÿã® head_phraseï¼ˆæœ€åˆã®æ•°è¡Œï¼‰ã€ã‚’ä½œã‚‹
    raw_head = extract_head_phrase(next_seg)
    if not raw_head:
        return -1, 0, "", [], ""

    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šã‚Šã€æœ«å°¾ã®å¥èª­ç‚¹ã‚’è½ã¨ã—ã¦ã‹ã‚‰ä½¿ã†
    base_head = raw_head.strip()
    base_head = base_head.rstrip("ã€‚ï¼Ÿï¼!ï¼Ÿï¼Œã€,.")

    if not base_head:
        return -1, 0, "", [], ""

    # è©¦ã™ãƒ•ãƒ¬ãƒ¼ã‚ºã®å€™è£œåˆ—ã‚’ä½œã‚‹
    candidates: List[str] = []
    seen: set[str] = set()
    shifted_heads: List[str] = []

    def add_candidate(s: str, is_shifted: bool = False):
        if not s:
            return
        if s in seen:
            return
        seen.add(s)
        candidates.append(s)
        if is_shifted:
            shifted_heads.append(s)

    # 1. ãã®ã¾ã¾ã® head_phrase
    add_candidate(base_head, is_shifted=False)

    # 2. å…ˆé ­ã‚’ 1 æ–‡å­—ãšã¤ãšã‚‰ã—ãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæœ€å¤§ HEAD_SHIFT_TRIES å›ï¼‰
    current = base_head
    for _ in range(HEAD_SHIFT_TRIES):
        if len(current) <= 1:
            break
        current = current[1:]
        add_candidate(current, is_shifted=True)

    # é †ç•ªã«ãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦ã™
    matched_phrase = ""
    for phrase in candidates:
        start_pos, size = _match_with_phrase(
            prev_seg, phrase, min_match_size, use_autojunk
        )
        if start_pos >= 0 and size > 0:
            matched_phrase = phrase
            return start_pos, size, base_head, shifted_heads, matched_phrase

    # ã©ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸ
    return -1, 0, base_head, shifted_heads, ""


# ============================================================
# çµåˆå‡¦ç†ï¼ˆå‰åŠå´ã ã‘ãƒãƒ¼ã‚¯ï¼‰ï¼‹ãƒ­ã‚°å‡ºåŠ›
# ============================================================

def build_merged_text(
    text: str,
    min_match_size: int,
    use_autojunk: bool,
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    ã¤ãªãç›®ã”ã¨ã«å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœ«å°¾ã®é‡è¤‡é–‹å§‹ä½ç½®ã‚’æ¢ã—ã€
    ãã“ã«ã€Œ-----ã“ã“ã‹ã‚‰é‡è¤‡-----ã€ã‚’æŒ¿å…¥ã™ã‚‹ã€‚

    - merged_text: ãƒãƒ¼ã‚«ãƒ¼æŒ¿å…¥æ¸ˆã¿ã®å…¨æ–‡
    - logs: ã¤ãªãç›®ã”ã¨ã®æ¤œå‡ºçµæœï¼ˆæˆåŠŸ/å¤±æ•—ã‚’å«ã‚€ï¼‰

    å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ãã®ã¾ã¾ï¼ˆã€Œã“ã“ã¾ã§é‡è¤‡ã€ã¯ä»˜ã‘ãªã„ï¼‰ã€‚
    """
    segments, markers = split_by_markers(text)

    if len(segments) <= 1 or not markers:
        # ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ­ã‚°ã¯ç©ºï¼‰
        return text, []

    merged: List[str] = []
    logs: List[Dict[str, Any]] = []

    # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¸€æ—¦ãã®ã¾ã¾å…¥ã‚Œã¦ãŠã
    merged.append(segments[0])

    for idx, marker in enumerate(markers):
        prev_seg = segments[idx]
        next_seg = segments[idx + 1]

        # é‡è¤‡é–‹å§‹ä½ç½®ã‚’æ¢ç´¢ï¼ˆå†…éƒ¨ã§ã‚­ãƒ¼æ–‡ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚‚è¡Œã†ï¼‰
        start_pos, size, base_head, shifted_heads, matched_phrase = find_overlap_start(
            prev_seg, next_seg, min_match_size, use_autojunk
        )

        if start_pos < 0 or size <= 0:
            # â˜… å¤±æ•—ãƒ­ã‚°ã‚’è¿½åŠ 
            logs.append(
                {
                    "ã¤ãªãç›®ç•ªå·": idx,
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": marker.get("file_name", ""),
                    "æ¤œå‡ºçµæœ": "è¦‹ã¤ã‹ã‚‰ãš",
                    "é–‹å§‹ä½ç½®": None,
                    "ä¸€è‡´æ–‡å­—æ•°": 0,
                    "head_phrase": base_head,
                    "shifted_phrases": shifted_heads,
                    "matched_phrase": "",
                }
            )

            merged.append("\n" + marker["marker_text"] + "\n")
            merged.append(next_seg)
            continue

        # â˜… æˆåŠŸãƒ­ã‚°ã‚’è¿½åŠ 
        logs.append(
            {
                "ã¤ãªãç›®ç•ªå·": idx,
                "ãƒ•ã‚¡ã‚¤ãƒ«å": marker.get("file_name", ""),
                "æ¤œå‡ºçµæœ": "æ¤œå‡º",
                "é–‹å§‹ä½ç½®": start_pos,
                "ä¸€è‡´æ–‡å­—æ•°": size,
                "head_phrase": base_head,
                "shifted_phrases": shifted_heads,
                "matched_phrase": matched_phrase,
            }
        )

        # å‰åŠå´ã®é‡è¤‡é–‹å§‹ä½ç½®ã§ã‚¿ã‚°ã‚’æŒ¿å…¥ã—ãŸæ–°ã—ã„å‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½œã‚‹
        new_prev = (
            prev_seg[:start_pos]
            + "\n" + BEGIN_TAG + "\n"
            + prev_seg[start_pos:]
        )

        # ç›´å‰ã® merged[-1]ï¼ˆå¤ã„ prev_segï¼‰ã‚’å·®ã—æ›¿ãˆ
        merged[-1] = new_prev

        # ãƒãƒ¼ã‚«ãƒ¼è¡Œ
        merged.append("\n" + marker["marker_text"] + "\n")

        # å¾ŒåŠå´ã¯ä½•ã‚‚å‰Šã‚‰ãšã€ãã®ã¾ã¾ç¶šã‘ã‚‹
        merged.append(next_seg)

    return "".join(merged), logs


# ============================================================
# Streamlit UI
# ============================================================

st.set_page_config(
    page_title="ğŸ“ æ–‡å­—èµ·ã“ã—çµåˆï¼ˆé‡è¤‡æ¤œå‡ºï¼šå¾ŒåŠã®æœ€åˆã®æ•°è¡Œãƒ™ãƒ¼ã‚¹ï¼‰",
    page_icon="ğŸ“",
    layout="wide",
)

st.title("ğŸ“ é‡è¤‡ç®‡æ‰€æ¤œå‡º")

st.markdown(
    """
- å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã® **ã€Œæœ€åˆã®æ•°è¡Œã€** ã‚’ã‚­ãƒ¼ã«ã—ã¦ã€  
  å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã® **ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ã€** ã®ä¸­ã‹ã‚‰  
  ã‚‚ã£ã¨ã‚‚ã‚ˆãä¸€è‡´ã™ã‚‹ä½ç½®ã‚’æ¢ã—ã€å‰åŠå´ã«ã ã‘  
  `-----ã“ã“ã‹ã‚‰é‡è¤‡-----` ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚

- å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã¯ã€Œã“ã“ã¾ã§é‡è¤‡éƒ¨åˆ†ã€ã¯æŒ¿å…¥ã—ã¾ã›ã‚“ã€‚
"""
)

# ğŸ”§ ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("æ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    min_match_size = st.slider(
        "ä¸€è‡´ã¨ã¿ãªã™æœ€ä½æ–‡å­—æ•°ï¼ˆMIN_MATCH_SIZEï¼‰",
        min_value=5,
        max_value=40,
        value=DEFAULT_MIN_MATCH_SIZE,
        step=1,
        help="é‡è¤‡ã¨ã¿ãªã™é€£ç¶šä¸€è‡´ã®æœ€ä½æ–‡å­—æ•°ã§ã™ã€‚å€¤ã‚’å°ã•ãã™ã‚‹ã¨æ¤œå‡ºãŒã‚†ã‚‹ããªã‚Šã€å¤§ããã™ã‚‹ã¨å³ã—ããªã‚Šã¾ã™ã€‚",
    )

    autojunk_option = st.radio(
        "autojunkï¼ˆSequenceMatcher è‡ªå‹•ã‚¸ãƒ£ãƒ³ã‚¯åˆ¤å®šï¼‰",
        options=["ONï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰", "OFFï¼ˆçŸ­ã„æ–‡ã§ã‚‚ç²¾ç¢ºã«ï¼‰"],
        index=0,
        help=(
            "ON: Pythonæ¨™æº–ã®è‡ªå‹•ã‚¸ãƒ£ãƒ³ã‚¯åˆ¤å®šã‚’ä½¿ã„ã¾ã™ï¼ˆé«˜é€Ÿã ãŒçŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’è½ã¨ã™ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚\n"
            "OFF: çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã®ä¸€è‡´ã‚‚å–ã‚Šã“ã¼ã—ã«ãããªã‚Šã¾ã™ãŒã€ã‚ãšã‹ã«é…ããªã‚Šã¾ã™ã€‚"
        ),
    )
    use_autojunk = autojunk_option.startswith("ON")

# ğŸ”½ ãƒ­ã‚¸ãƒƒã‚¯èª¬æ˜ï¼ˆåˆæœŸã¯ç•³ã‚“ã§ãŠãï¼‰
with st.expander("ğŸ” ã“ã®ãƒ„ãƒ¼ãƒ«ã®é‡è¤‡æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ¦‚è¦ï¼‰", expanded=False):
    st.markdown(
        f"""
1. **ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²**  
   - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’ã€Œ--- ã“ã“ãŒã¤ãªãç›®ã§ã™ï¼ˆâ€¦ï¼‰ã€ã¨ã„ã†ãƒãƒ¼ã‚«ãƒ¼è¡Œã§åˆ†å‰²ã—ã€  
     `[å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆ][ãƒãƒ¼ã‚«ãƒ¼][å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆ]...` ã¨ã„ã†å½¢ã«åˆ†ã‘ã¾ã™ã€‚

2. **å¾ŒåŠå´ã®ã€Œæœ€åˆã®æ•°è¡Œã€ã‚’æŠ½å‡º**  
   - å„ã¤ãªãç›®ã«ã¤ã„ã¦ã€å¾ŒåŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å…ˆé ­ã‹ã‚‰ `HEAD_CHARS` æ–‡å­—ã‚’å–ã‚Šå‡ºã—ã€  
     ã€Œã€‚ã€ã€Œï¼Ÿã€ã€Œï¼ã€ã€Œæ”¹è¡Œã€ãªã©ã§åŒºåˆ‡ã£ã¦ **æœ€å¤§ `HEAD_SENTENCES` æ–‡** ã¾ã§ã‚’  
     head_phrase ã¨ã—ã¦ä½¿ã„ã¾ã™ï¼ˆæœ«å°¾ã®å¥èª­ç‚¹ã¯é™¤å»ï¼‰ã€‚

3. **ã‚­ãƒ¼æ–‡ã®ã‚¹ãƒ©ã‚¤ãƒ‰**  
   - head_phrase ãã®ã‚‚ã®ã«åŠ ãˆã€å…ˆé ­ã‚’1æ–‡å­—ãšã¤å‰Šã£ãŸ `{HEAD_SHIFT_TRIES}` å€‹ã®å€™è£œã‚‚è©¦ã—ã¾ã™ã€‚

4. **å‰åŠå´ã®ã€Œæœ€å¾Œã®2åˆ†ç›¸å½“ã€ã‚’å–å¾—**  
   - å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ«å°¾ã‹ã‚‰ `OVERLAP_CHARS` æ–‡å­—ã ã‘ã‚’åˆ‡ã‚Šå‡ºã—ã€ã“ã‚Œã‚’æ¯”è¼ƒå¯¾è±¡ã¨ã—ã¾ã™ã€‚

5. **æ­£è¦åŒ–ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—**  
   - å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãƒ»æ”¹è¡Œãƒ»å¥èª­ç‚¹ãƒ»åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ã—ã¦ã‹ã‚‰ã€`SequenceMatcher` ã§ä¸€è‡´ãƒ–ãƒ­ãƒƒã‚¯ã‚’èª¿ã¹ã¾ã™ã€‚  
   - autojunk ã® ON/OFF ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã™ã€‚

6. **ã‚‚ã£ã¨ã‚‚è‡ªç„¶ãªä¸€è‡´ä½ç½®ã‚’é¸ã³ã€ã‚¿ã‚°ã‚’æŒ¿å…¥**  
   - é€£ç¶šä¸€è‡´ãŒ `{min_match_size}` æ–‡å­—ä»¥ä¸Šã‚ã‚‹éƒ¨åˆ†ã‹ã‚‰ã€è‡ªç„¶ãªä½ç½®ã‚’1ç®‡æ‰€é¸ã³ã€  
     å‰åŠã‚»ã‚°ãƒ¡ãƒ³ãƒˆå´ã«ã®ã¿ `-----ã“ã“ã‹ã‚‰é‡è¤‡-----` è¡Œã‚’æŒ¿å…¥ã—ã¾ã™ã€‚
        """
    )

uploaded_files = st.file_uploader(
    "æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ (.txt) ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["txt"],
    accept_multiple_files=True,
)

run = st.button("â–¶ï¸ é‡è¤‡ç®‡æ‰€æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹", type="primary")

if run:
    if not uploaded_files:
        st.warning("å…ˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    else:
        for up in uploaded_files:
            name = up.name
            raw = up.read()
            try:
                text = raw.decode("utf-8")
            except UnicodeDecodeError:
                text = raw.decode("cp932", errors="replace")

            # â˜… çµåˆãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ­ã‚°ã‚’å—ã‘å–ã‚‹
            merged, logs = build_merged_text(text, min_match_size, use_autojunk)

            st.subheader(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {name}")

            # ğŸ” ã¤ãªãç›®ã”ã¨ã®æ¤œå‡ºçµæœï¼ˆæ¦‚è¦ï¼‰
            with st.expander("ğŸ” ã¤ãªãç›®ã”ã¨ã®æ¤œå‡ºãƒ­ã‚°ï¼ˆæ¦‚è¦ï¼‰", expanded=True):
                if logs:
                    st.table(
                        [
                            {
                                "ã¤ãªãç›®ç•ªå·": item["ã¤ãªãç›®ç•ªå·"],
                                "ãƒ•ã‚¡ã‚¤ãƒ«å": item["ãƒ•ã‚¡ã‚¤ãƒ«å"],
                                "æ¤œå‡ºçµæœ": item["æ¤œå‡ºçµæœ"],
                                "é–‹å§‹ä½ç½®": item["é–‹å§‹ä½ç½®"],
                                "ä¸€è‡´æ–‡å­—æ•°": item["ä¸€è‡´æ–‡å­—æ•°"],
                            }
                            for item in logs
                        ]
                    )
                else:
                    st.info("ã¤ãªãç›®ãƒãƒ¼ã‚«ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ãƒ­ã‚°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

            # ğŸ§© head_phrase ã¨ shifted_phrases ã®è©³ç´°
            with st.expander("ğŸ§© head_phrase ã¨ shifted_phrases ã®è©³ç´°", expanded=False):
                if not logs:
                    st.info("ãƒ­ã‚°ãŒãªã„ãŸã‚è¡¨ç¤ºã§ãã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    for item in logs:
                        st.markdown(
                            f"### ğŸ”¹ ã¤ãªãç›® {item['ã¤ãªãç›®ç•ªå·']} â€” {item['æ¤œå‡ºçµæœ']}"
                        )
                        st.markdown("**ğŸ”¸ head_phraseï¼ˆæ•´å½¢å¾Œã®ã‚­ãƒ¼æ–‡ï¼‰**")
                        st.code(item["head_phrase"] or "ï¼ˆç©ºã§ã™ï¼‰")

                        st.markdown("**ğŸ”¸ shifted_phrasesï¼ˆå…ˆé ­ã‚’1æ–‡å­—ãšã¤ãšã‚‰ã—ãŸå€™è£œï¼‰**")
                        if item["shifted_phrases"]:
                            for s in item["shifted_phrases"]:
                                st.code(s)
                        else:
                            st.write("ï¼ˆshifted_phrases ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")

                        st.markdown("**ğŸ”¸ matched_phraseï¼ˆå®Ÿéš›ã«ãƒãƒƒãƒã—ãŸãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰**")
                        st.code(item["matched_phrase"] or "ï¼ˆãƒãƒƒãƒãªã—ï¼‰")

                        st.markdown("---")

            # ğŸ“˜ ãƒ†ã‚­ã‚¹ãƒˆçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“˜ çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.text(merged)

            st.download_button(
                "ğŸ“¥ çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.txt)",
                merged.encode("utf-8"),
                file_name=f"{name.rsplit('.',1)[0]}_é‡è¤‡ç®‡æ‰€æ¤œå‡º.txt",
                mime="text/plain",
            )

        st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
